# Feature-Based Exploration Scenario
#
# Tests exploration through feature-oriented questions.
# "How does X work?" is a common query pattern for agents
# trying to understand functionality without knowing implementation details.

[scenario]
id = "zed-feature-exploration"
name = "Feature-Based Exploration"
repo = "zed"
difficulty = "medium"
description = "Explore how Zed handles file saving through natural feature questions"

[task]
prompt = "Understand how Zed handles file saving"
intent = "architectural_discovery"

[expected]
must_find_files = [
    "**/language/src/buffer.rs",
    "**/worktree/src/worktree.rs",
    "**/project/src/project.rs",
    "**/project/src/buffer_store.rs",
]
must_find_symbols = [
    "save",
    "Buffer",
    "is_dirty",
    "has_unsaved_edits",
    "write_file",
    "did_save",
    "BufferSnapshot",
    "DiskState",
]
noise_patterns = [
    "**/tests/**",
    "**/test/**",
    "test_*",
    "Mock*",
    "**/fixtures/**",
]

# Step 1: Start with a natural feature question
[[steps]]
query = "How does file saving work?"
expand_top = 4
expected_results = 5
max_noise_ratio = 0.3
scope = "code"

# Step 2: Understand what triggers saves
[[steps]]
query = "What triggers a save operation?"
depends_on_previous = true
expected_results = 4
scope = "code"

# Step 3: Understand dirty tracking
[[steps]]
query = "How are unsaved changes tracked?"
depends_on_previous = true
expected_results = 3
scope = "code"

# Step 4: Follow up on discovered buffer type
[[steps]]
query = "How does {{previous.symbol}} interact with the file system?"
depends_on_previous = true
expected_results = 3
scope = "code"

[success]
min_discovery_score = 0.5
max_noise_ratio = 0.3
max_steps_to_core = 3
min_file_diversity = 0.5

# LLM-as-judge comprehension evaluation
[llm_judge]
min_comprehension_score = 0.5

[[llm_judge.comprehension_questions]]
question = "How does Zed track whether a file has unsaved changes?"
expected_concepts = ["Buffer", "is_dirty", "has_unsaved_edits", "saved_version", "version"]
wrong_concepts = ["autosave timer", "polling"]
weight = 1.0

[[llm_judge.comprehension_questions]]
question = "What happens when a user saves a file in Zed?"
expected_concepts = ["Buffer", "write_file", "Worktree", "did_save", "BufferStore"]
wrong_concepts = []
weight = 1.5

[[llm_judge.comprehension_questions]]
question = "What is the role of BufferSnapshot in the save process?"
expected_concepts = ["BufferSnapshot", "immutable", "version", "text"]
wrong_concepts = []
weight = 1.0
