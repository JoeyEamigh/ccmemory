//! Markdown report generation.

use crate::Result;
use crate::metrics::MetricTargets;
use crate::scenarios::ScenarioResult;
use chrono::Utc;
use std::fmt::Write as _;
use std::path::Path;

/// Markdown report generator.
pub struct MarkdownReport {
  content: String,
}

impl MarkdownReport {
  /// Create a markdown report from scenario results.
  pub fn from_results(results: &[ScenarioResult]) -> Self {
    let mut content = String::new();

    Self::write_header(&mut content, results);
    Self::write_summary(&mut content, results);
    Self::write_performance_table(&mut content, results);
    Self::write_accuracy_table(&mut content, results);
    Self::write_exploration_table(&mut content, results);
    Self::write_comprehension_table(&mut content, results);
    Self::write_scenario_details(&mut content, results);

    Self { content }
  }

  fn write_header(out: &mut String, results: &[ScenarioResult]) {
    let _ = writeln!(out, "# CCEngram Benchmark Report");
    let _ = writeln!(out);
    let _ = writeln!(out, "**Generated:** {}", Utc::now().format("%Y-%m-%d %H:%M:%S UTC"));
    let _ = writeln!(out, "**Version:** {}", env!("CARGO_PKG_VERSION"));
    let _ = writeln!(out, "**Scenarios:** {}", results.len());
    let _ = writeln!(out);
  }

  fn write_summary(out: &mut String, results: &[ScenarioResult]) {
    let passed = results.iter().filter(|r| r.passed).count();
    let failed = results.len() - passed;
    let pass_rate = if results.is_empty() {
      0.0
    } else {
      passed as f64 / results.len() as f64 * 100.0
    };

    let total_time: u64 = results.iter().map(|r| r.total_duration_ms).sum();

    let _ = writeln!(out, "## Summary");
    let _ = writeln!(out);
    let _ = writeln!(out, "| Metric | Value |");
    let _ = writeln!(out, "|--------|-------|");
    let _ = writeln!(
      out,
      "| **Pass Rate** | {:.1}% ({}/{}) |",
      pass_rate,
      passed,
      results.len()
    );
    let _ = writeln!(out, "| **Passed** | {} |", passed);
    let _ = writeln!(out, "| **Failed** | {} |", failed);
    let _ = writeln!(out, "| **Total Time** | {:.2}s |", total_time as f64 / 1000.0);
    let _ = writeln!(out);

    // Pass/fail emoji summary
    let _ = writeln!(out, "### Quick Status");
    let _ = writeln!(out);
    for result in results {
      let emoji = if result.passed { "✅" } else { "❌" };
      let _ = writeln!(out, "- {} {} ({})", emoji, result.scenario_name, result.scenario_id);
    }
    let _ = writeln!(out);
  }

  fn write_performance_table(out: &mut String, results: &[ScenarioResult]) {
    let _ = writeln!(out, "## Performance");
    let _ = writeln!(out);
    let _ = writeln!(out, "| Scenario | Search p50 | Search p95 | Context p50 | Total |");
    let _ = writeln!(out, "|----------|------------|------------|-------------|-------|");

    for result in results {
      let _ = writeln!(
        out,
        "| {} | {}ms | {}ms | {}ms | {:.2}s |",
        result.scenario_id,
        result.performance.search_latency.p50_ms,
        result.performance.search_latency.p95_ms,
        result.performance.context_latency.p50_ms,
        result.total_duration_ms as f64 / 1000.0
      );
    }
    let _ = writeln!(out);
  }

  fn write_accuracy_table(out: &mut String, results: &[ScenarioResult]) {
    let targets = MetricTargets::default();

    let _ = writeln!(out, "## Accuracy");
    let _ = writeln!(out);
    let _ = writeln!(
      out,
      "| Scenario | File Recall | Symbol Recall | MRR | Noise | Steps | First Relevant |"
    );
    let _ = writeln!(
      out,
      "|----------|-------------|---------------|-----|-------|-------|----------------|"
    );

    for result in results {
      let file_icon = if result.accuracy.file_recall >= targets.file_recall {
        "✅"
      } else {
        "❌"
      };
      let symbol_icon = if result.accuracy.symbol_recall >= targets.symbol_recall {
        "✅"
      } else {
        "❌"
      };
      let mrr_icon = if result.accuracy.mrr >= targets.mrr {
        "✅"
      } else {
        "❌"
      };
      let noise_icon = if result.accuracy.noise_ratio <= targets.noise_ratio {
        "✅"
      } else {
        "❌"
      };
      let steps_icon = result
        .accuracy
        .steps_to_core
        .map(|s| if s <= targets.max_steps_to_core { "✅" } else { "❌" })
        .unwrap_or("➖");

      let steps_str = result
        .accuracy
        .steps_to_core
        .map(|s: usize| s.to_string())
        .unwrap_or_else(|| "N/A".to_string());

      let first_relevant_str = result
        .accuracy
        .time_to_first_relevant_ms
        .map(|ms| format!("{}ms", ms))
        .unwrap_or_else(|| "N/A".to_string());

      let _ = writeln!(
        out,
        "| {} | {} {:.0}% | {} {:.0}% | {} {:.2} | {} {:.0}% | {} {} | {} |",
        result.scenario_id,
        file_icon,
        result.accuracy.file_recall * 100.0,
        symbol_icon,
        result.accuracy.symbol_recall * 100.0,
        mrr_icon,
        result.accuracy.mrr,
        noise_icon,
        result.accuracy.noise_ratio * 100.0,
        steps_icon,
        steps_str,
        first_relevant_str
      );
    }
    let _ = writeln!(out);

    // Targets legend
    let _ = writeln!(
      out,
      "**Targets:** File Recall ≥{:.0}%, Symbol Recall ≥{:.0}%, MRR ≥{:.1}, Noise ≤{:.0}%, Steps ≤{}",
      targets.file_recall * 100.0,
      targets.symbol_recall * 100.0,
      targets.mrr,
      targets.noise_ratio * 100.0,
      targets.max_steps_to_core
    );
    let _ = writeln!(out);
  }

  fn write_exploration_table(out: &mut String, results: &[ScenarioResult]) {
    let targets = MetricTargets::default();

    let _ = writeln!(out, "## Exploration Quality");
    let _ = writeln!(out);
    let _ = writeln!(
      out,
      "| Scenario | Convergence | Nav Efficiency | Hint Utility | Context Bloat | Dead Ends | File Diversity |"
    );
    let _ = writeln!(
      out,
      "|----------|-------------|----------------|--------------|---------------|-----------|----------------|"
    );

    for result in results {
      let convergence_icon = if result.accuracy.convergence_rate >= targets.convergence_rate {
        "✅"
      } else if result.accuracy.convergence_rate >= targets.convergence_rate * 0.8 {
        "⚠️"
      } else {
        "❌"
      };

      let nav_icon = if result.accuracy.navigation_efficiency >= targets.navigation_efficiency {
        "✅"
      } else if result.accuracy.navigation_efficiency >= targets.navigation_efficiency * 0.8 {
        "⚠️"
      } else {
        "❌"
      };

      let hint_icon = if result.accuracy.hint_utility >= targets.hint_utility {
        "✅"
      } else if result.accuracy.hint_utility >= targets.hint_utility * 0.8 {
        "⚠️"
      } else {
        "❌"
      };

      // Context bloat: lower is better
      let bloat_icon = if result.accuracy.context_bloat <= targets.context_bloat {
        "✅"
      } else if result.accuracy.context_bloat <= targets.context_bloat * 1.2 {
        "⚠️"
      } else {
        "❌"
      };

      // Dead end ratio: lower is better
      let dead_end_icon = if result.accuracy.dead_end_ratio <= targets.dead_end_ratio {
        "✅"
      } else if result.accuracy.dead_end_ratio <= targets.dead_end_ratio * 1.2 {
        "⚠️"
      } else {
        "❌"
      };

      // File diversity: higher is better
      let diversity_icon = if result.accuracy.avg_file_diversity_top5 >= targets.file_diversity {
        "✅"
      } else if result.accuracy.avg_file_diversity_top5 >= targets.file_diversity * 0.8 {
        "⚠️"
      } else {
        "❌"
      };

      let _ = writeln!(
        out,
        "| {} | {} {:.0}% | {} {:.0}% | {} {:.0}% | {} {:.0}% | {} {:.0}% | {} {:.0}% |",
        result.scenario_id,
        convergence_icon,
        result.accuracy.convergence_rate * 100.0,
        nav_icon,
        result.accuracy.navigation_efficiency * 100.0,
        hint_icon,
        result.accuracy.hint_utility * 100.0,
        bloat_icon,
        result.accuracy.context_bloat * 100.0,
        dead_end_icon,
        result.accuracy.dead_end_ratio * 100.0,
        diversity_icon,
        result.accuracy.avg_file_diversity_top5 * 100.0
      );
    }
    let _ = writeln!(out);

    // Targets legend
    let _ = writeln!(
      out,
      "**Targets:** Convergence ≥{:.0}%, Nav Efficiency ≥{:.0}%, Hint Utility ≥{:.0}%, Context Bloat ≤{:.0}%, Dead Ends ≤{:.0}%, File Diversity ≥{:.0}%",
      targets.convergence_rate * 100.0,
      targets.navigation_efficiency * 100.0,
      targets.hint_utility * 100.0,
      targets.context_bloat * 100.0,
      targets.dead_end_ratio * 100.0,
      targets.file_diversity * 100.0
    );
    let _ = writeln!(out);
  }

  fn write_comprehension_table(out: &mut String, results: &[ScenarioResult]) {
    // Only show if any scenario has comprehension results
    let has_comprehension = results.iter().any(|r| r.comprehension.is_some());
    if !has_comprehension {
      return;
    }

    let _ = writeln!(out, "## Comprehension (LLM Judge)");
    let _ = writeln!(out);
    let _ = writeln!(out, "| Scenario | Score | Questions | Passed | Summary |");
    let _ = writeln!(out, "|----------|-------|-----------|--------|---------|");

    for result in results {
      match &result.comprehension {
        Some(comp) => {
          let score_icon = if comp.passed {
            "✅"
          } else if comp.overall_score >= 0.5 {
            "⚠️"
          } else {
            "❌"
          };

          let passed_str = if comp.passed { "Yes" } else { "No" };

          // Truncate summary if too long
          let summary = if comp.summary.len() > 50 {
            format!("{}...", &comp.summary[..47])
          } else {
            comp.summary.clone()
          };

          let _ = writeln!(
            out,
            "| {} | {} {:.0}% | {} | {} | {} |",
            result.scenario_id,
            score_icon,
            comp.overall_score * 100.0,
            comp.questions.len(),
            passed_str,
            summary
          );
        }
        None => {
          let _ = writeln!(
            out,
            "| {} | ➖ N/A | - | - | No comprehension questions defined |",
            result.scenario_id
          );
        }
      }
    }
    let _ = writeln!(out);
  }

  fn write_scenario_details(out: &mut String, results: &[ScenarioResult]) {
    let _ = writeln!(out, "## Scenario Details");
    let _ = writeln!(out);

    for result in results {
      let status = if result.passed { "✅ PASSED" } else { "❌ FAILED" };
      let _ = writeln!(out, "### {} - {}", result.scenario_id, status);
      let _ = writeln!(out);

      // Errors if any
      if !result.errors.is_empty() {
        let _ = writeln!(out, "**Errors:**");
        for error in &result.errors {
          let _ = writeln!(out, "- {}", error);
        }
        let _ = writeln!(out);
      }

      // Files found/missed
      if !result.accuracy.files_found.is_empty() || !result.accuracy.files_missed.is_empty() {
        let _ = writeln!(out, "**Files:**");
        if !result.accuracy.files_found.is_empty() {
          let _ = writeln!(out, "- Found: {}", result.accuracy.files_found.join(", "));
        }
        if !result.accuracy.files_missed.is_empty() {
          let _ = writeln!(out, "- Missed: {}", result.accuracy.files_missed.join(", "));
        }
        let _ = writeln!(out);
      }

      // Symbols found/missed
      if !result.accuracy.symbols_found.is_empty() || !result.accuracy.symbols_missed.is_empty() {
        let _ = writeln!(out, "**Symbols:**");
        if !result.accuracy.symbols_found.is_empty() {
          let _ = writeln!(out, "- Found: `{}`", result.accuracy.symbols_found.join("`, `"));
        }
        if !result.accuracy.symbols_missed.is_empty() {
          let _ = writeln!(out, "- Missed: `{}`", result.accuracy.symbols_missed.join("`, `"));
        }
        let _ = writeln!(out);
      }

      // Step breakdown
      if !result.steps.is_empty() {
        let _ = writeln!(out, "**Steps:**");
        let _ = writeln!(out);
        let _ = writeln!(out, "| # | Query | Results | Noise | Latency |");
        let _ = writeln!(out, "|---|-------|---------|-------|---------|");

        for step in &result.steps {
          let query_preview = if step.query.len() > 40 {
            format!("{}...", &step.query[..40])
          } else {
            step.query.clone()
          };

          let _ = writeln!(
            out,
            "| {} | {} | {} | {:.0}% | {}ms |",
            step.step_index + 1,
            query_preview,
            step.result_count,
            step.noise_ratio * 100.0,
            step.latency_ms
          );
        }
        let _ = writeln!(out);
      }
    }
  }

  /// Save to a markdown file.
  pub fn save(&self, path: &Path) -> Result<()> {
    std::fs::write(path, &self.content)?;
    Ok(())
  }

  /// Get the markdown content.
  pub fn content(&self) -> &str {
    &self.content
  }
}

#[cfg(test)]
mod tests {
  use super::*;
  use crate::metrics::{AccuracyMetrics, LatencyStats, PerformanceMetrics};
  use crate::scenarios::StepResult;
  use tempfile::TempDir;

  fn sample_result(id: &str, passed: bool) -> ScenarioResult {
    ScenarioResult {
      scenario_id: id.to_string(),
      scenario_name: format!("Test {}", id),
      passed,
      performance: PerformanceMetrics {
        search_latency: LatencyStats {
          min_ms: 50,
          max_ms: 200,
          mean_ms: 100,
          p50_ms: 90,
          p95_ms: 180,
          p99_ms: 195,
          count: 5,
        },
        context_latency: LatencyStats::default(),
        total_time_ms: 500,
        steps: vec![],
        peak_memory_bytes: None,
        avg_cpu_percent: None,
      },
      accuracy: AccuracyMetrics {
        file_recall: 0.8,
        symbol_recall: 0.75,
        steps_to_core: Some(2),
        mrr: 0.6,
        noise_ratio: 0.15,
        top3_noise: 0.0,
        hint_utility: 0.7,
        suggestion_quality: 0.5,
        convergence_rate: 0.85,
        avg_info_gain: 0.4,
        context_bloat: 0.1,
        navigation_efficiency: 0.7,
        dead_end_ratio: 0.1,
        context_budget_efficiency: 0.8,
        total_bytes_returned: 10000,
        useful_bytes_returned: 8000,
        max_consecutive_failures: 1,
        rabbit_hole_steps: 0,
        rabbit_hole_ratio: 0.0,
        time_to_first_relevant_ms: Some(150),
        avg_file_diversity_top5: 0.8,
        files_found: vec!["found.rs".to_string()],
        files_missed: vec!["missed.rs".to_string()],
        symbols_found: vec!["Found".to_string()],
        symbols_missed: vec!["Missed".to_string()],
      },
      steps: vec![StepResult {
        step_index: 0,
        query: "test query".to_string(),
        result_count: 5,
        noise_ratio: 0.2,
        result_ids: vec![],
        files_found: vec![],
        symbols_found: vec![],
        callers: vec![],
        callees: vec![],
        latency_ms: 100,
        passed: true,
      }],
      errors: vec![],
      total_duration_ms: 500,
      comprehension: None,
    }
  }

  #[test]
  fn test_markdown_generation() {
    let results = vec![sample_result("test-1", true), sample_result("test-2", false)];

    let report = MarkdownReport::from_results(&results);
    let content = report.content();

    assert!(content.contains("# CCEngram Benchmark Report"));
    assert!(content.contains("## Summary"));
    assert!(content.contains("## Performance"));
    assert!(content.contains("## Accuracy"));
    assert!(content.contains("test-1"));
    assert!(content.contains("✅"));
    assert!(content.contains("❌"));
  }

  #[test]
  fn test_save_markdown() {
    let temp = TempDir::new().unwrap();
    let path = temp.path().join("report.md");

    let results = vec![sample_result("test-1", true)];
    let report = MarkdownReport::from_results(&results);

    report.save(&path).unwrap();
    assert!(path.exists());

    let content = std::fs::read_to_string(&path).unwrap();
    assert!(content.contains("# CCEngram Benchmark Report"));
  }

  #[test]
  fn test_empty_results() {
    let report = MarkdownReport::from_results(&[]);
    let content = report.content();

    assert!(content.contains("# CCEngram Benchmark Report"));
    assert!(content.contains("0%"));
  }
}
