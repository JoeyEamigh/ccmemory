# CCEngram Benchmark Harness

Comprehensive benchmarking for testing the exploration capabilities of CCEngram's `explore` and `context` tools against large real-world codebases.

## Overview

The benchmark harness tests how well CCEngram helps agents discover important code without prior context. Unlike search benchmarks that test finding known items, this tests **exploration** - the ability to navigate unfamiliar codebases and find architecturally significant code.

## Quick Start

```bash
# Build the benchmark tool
cargo build -p benchmark

# Download and cache test repositories
cargo run -p benchmark -- index --repos zed,vscode

# List available scenarios
cargo run -p benchmark -- list --detailed

# Run all benchmarks
cargo run -p benchmark -- run --output ./results

# Run specific scenarios (glob patterns supported)
cargo run -p benchmark -- run --scenarios "zed*" --output ./results

# Run in parallel (faster, less detailed progress)
cargo run -p benchmark -- run --parallel --output ./results

# Compare two runs for regressions
cargo run -p benchmark -- compare baseline.json current.json --threshold 10
```

## CLI Commands

### `run` - Execute Benchmark Scenarios

```bash
ccengram-bench run [OPTIONS]

Options:
  -o, --output <DIR>         Output directory for results [default: ./benchmark-results]
  -s, --scenarios <PATTERN>  Filter scenarios by glob pattern
      --parallel             Run scenarios concurrently
      --llm-judge            Enable LLM-as-judge evaluation
      --scenarios-dir <DIR>  Custom scenarios directory
      --name <NAME>          Name for this benchmark run
```

### `compare` - Regression Detection

```bash
ccengram-bench compare <BASELINE> <CURRENT> [OPTIONS]

Arguments:
  <BASELINE>  Baseline results JSON file
  <CURRENT>   Current results JSON file

Options:
  -t, --threshold <PCT>  Regression threshold percentage [default: 10]
  -o, --output <FILE>    Save comparison report
```

### `index` - Prepare Repositories

```bash
ccengram-bench index [OPTIONS]

Options:
  -r, --repos <LIST>      Repositories to prepare: zed, vscode, or 'all' [default: all]
      --force             Force re-download even if cached
      --cache-dir <DIR>   Custom cache directory
```

### `index-perf` - Initial Indexing Performance

```bash
ccengram-bench index-perf [OPTIONS]

Options:
  -r, --repos <LIST>      Repositories to benchmark: zed, vscode, or 'all' [default: all]
  -i, --iterations <N>    Number of iterations per repo [default: 3]
  -o, --output <DIR>      Output directory for results [default: ./benchmark-results]
      --cold              Clear index between iterations (cold start testing)
      --cache-dir <DIR>   Custom cache directory
```

Measures initial indexing performance including scan time, chunking throughput, and resource usage.

### `list` - Show Available Scenarios

```bash
ccengram-bench list [OPTIONS]

Options:
  -d, --detailed          Show full scenario details
      --scenarios-dir     Custom scenarios directory
```

### `clean` - Remove Cached Data

```bash
ccengram-bench clean [OPTIONS]

Options:
  --all           Clean all cached data
  --repo <NAME>   Clean specific repository cache
```

## Target Repositories

| Repository | Language | Size | Use Case |
|------------|----------|------|----------|
| **Zed** | Rust | ~1M LOC | Editor architecture, commands, LSP integration |
| **VSCode** | TypeScript | ~1M LOC | Large codebase stress test, extension system |

Both are editor codebases with complex architectural discovery scenarios.

## Built-in Scenarios

| Scenario | Repo | Pattern | Difficulty | Description |
|----------|------|---------|------------|-------------|
| `zed-blind-exploration` | zed | Blind | hard | Explore from zero knowledge |
| `zed-command-system` | zed | Architectural | hard | Command/action system discovery |
| `zed-lsp-integration` | zed | Architectural | hard | LSP integration patterns |
| `zed-feature-exploration` | zed | Feature | medium | "How does file saving work?" |
| `zed-location-exploration` | zed | Location | medium | "Where to add new commands?" |
| `zed-pattern-exploration` | zed | Pattern | hard | HTTP/external service patterns |
| `vscode-blind-exploration` | vscode | Blind | hard | Explore from zero knowledge |
| `vscode-editor-core` | vscode | Architectural | hard | Core editor discovery |
| `vscode-extension-system` | vscode | Architectural | hard | Extension API patterns |

Run specific scenarios with glob patterns:
```bash
cargo run -p benchmark -- run --scenarios "zed-feature*"
cargo run -p benchmark -- run --scenarios "*blind*"
```

## Scenario Definition Format

Scenarios are defined in TOML files in `crates/benchmark/scenarios/`:

```toml
[scenario]
id = "zed-command-system"
name = "Understanding Zed Command Architecture"
repo = "zed"
difficulty = "hard"  # easy, medium, hard

[task]
prompt = "How does Zed handle editor commands?"
intent = "architectural_discovery"  # or: symbol_lookup, flow_tracing, bug_investigation

[expected]
must_find_files = [
    "**/commands.rs",
    "**/actions.rs",
    "**/keymap/**",
]
must_find_symbols = ["Action", "actions", "dispatch", "Keymap", "KeyBinding"]
noise_patterns = ["**/tests/**", "test_*", "Mock*"]

[[steps]]
query = "How does Zed handle editor commands and actions?"
expected_results = 5
max_noise_ratio = 0.3
scope = "code"  # code, memory, docs, all
expand_top = 3  # Include full context for top N results (default: 3)

[[steps]]
query = "What is the Action type and how is it dispatched?"
depends_on_previous = true
expand_top = 5  # More context when following up

[success]
min_discovery_score = 0.7       # File recall target
max_noise_ratio = 0.25          # Maximum acceptable noise
max_steps_to_core = 3           # Steps to find first core result
min_convergence_rate = 0.7      # How quickly discoveries plateau
max_context_bloat = 0.3         # Max % of useless context calls
min_navigation_efficiency = 0.5 # optimal/actual hops
min_suggestion_quality = 0.5    # % of useful suggestions
max_dead_end_ratio = 0.2        # Max % of wasted queries
max_time_to_first_relevant_ms = 5000  # Max time to first useful result
min_file_diversity = 0.6        # Min unique files in top-5 results (0.6 = 3/5)
```

## Metrics

### Indexing Performance Metrics

| Metric | Description |
|--------|-------------|
| **Scan Duration** | Time to scan repository files |
| **Index Duration** | Time to chunk, embed, and store |
| **Files/Second** | Indexing throughput |
| **Peak Memory** | Maximum memory during indexing |

### Search Performance Metrics

| Metric | Description |
|--------|-------------|
| **Search Latency** | p50/p95/p99 latency for explore queries |
| **Context Latency** | p50/p95/p99 latency for context fetches |
| **Total Time** | End-to-end scenario execution time |
| **Peak Memory** | Maximum memory usage during execution |
| **Avg CPU** | Average CPU utilization |

### Accuracy Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **File Recall** | % of must-find files discovered | >= 70% |
| **Symbol Recall** | % of must-find symbols discovered | >= 70% |
| **Steps to Core** | Queries needed to find first core result | <= 3 |
| **MRR** | Mean reciprocal rank of first correct result | >= 0.5 |
| **Noise Ratio** | % of results matching noise patterns | <= 25% |
| **Top-3 Noise** | Noise in top 3 results | <= 10% |
| **Hint Utility** | % of callers/callees that are relevant | >= 60% |
| **Suggestion Quality** | % of suggestions leading to useful results | >= 50% |
| **Time to First Relevant** | Milliseconds until first useful result found | <= 5000ms |
| **File Diversity (Top-5)** | Unique files / top-5 results (1.0 = all different files) | >= 60% |

### Exploration Quality Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Convergence Rate** | How quickly discoveries plateau (1.0 = finds things early) | >= 70% |
| **Navigation Efficiency** | optimal_hops / actual_hops to reach targets | >= 50% |
| **Context Bloat** | % of context calls that provided no new information | <= 30% |
| **Dead End Ratio** | % of queries that found nothing useful | <= 20% |
| **Info Gain** | Average new discoveries per step | >= 0.3 |

### Context Budget Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Context Budget Efficiency** | useful_bytes / total_bytes returned | >= 50% |
| **Total Bytes Returned** | Cumulative bytes across all explore/context calls | - |
| **Useful Bytes** | Bytes containing expected symbols/files | - |

### Path-Based Failure Metrics (Rabbit Holes)

| Metric | Description | Target |
|--------|-------------|--------|
| **Max Consecutive Failures** | Longest streak of steps without finding expected items | <= 3 |
| **Rabbit Hole Steps** | Total steps spent in rabbit holes (2+ consecutive failures) | <= 2 |
| **Rabbit Hole Ratio** | % of steps spent in rabbit holes | <= 20% |

### LLM Comprehension Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Comprehension Score** | Weighted average of question scores | >= 60% |
| **Concepts Found** | % of expected concepts mentioned in answers | >= 70% |
| **Wrong Concepts** | Incorrect concepts indicating misunderstanding | 0 |

## Adaptive Exploration Templates

Steps can use templates to build queries based on previous step results:

```toml
[[steps]]
query = "What does {{previous.symbol}} do?"
depends_on_previous = true

[[steps]]
query = "Show me the implementation of {{previous.symbols[0]}}"
context_ids = ["{{previous.id}}"]
```

Available templates:
- `{{previous.symbol}}` - First symbol from previous step
- `{{previous.symbols[N]}}` - Nth symbol from previous step
- `{{previous.file}}` - First file from previous step
- `{{previous.files[N]}}` - Nth file from previous step
- `{{previous.id}}` - First result ID from previous step
- `{{previous.caller}}` - First caller symbol from previous context
- `{{previous.callee}}` - First callee symbol from previous context

## Blind Exploration Scenarios

For testing true discovery capability without prior knowledge:

```toml
[scenario]
id = "zed-blind-exploration"
name = "Blind Exploration of Zed Editor"
difficulty = "hard"

[[steps]]
# Generic questions - no codebase-specific terms
query = "What is the main entry point of this application?"

[[steps]]
query = "How is the user interface organized?"
depends_on_previous = true

[[steps]]
# Adaptive template - follow what was discovered
query = "What does {{previous.symbol}} do?"
depends_on_previous = true
```

## LLM-as-Judge Comprehension Testing

Test whether exploration results enable understanding:

```toml
[llm_judge]
min_comprehension_score = 0.6

[[llm_judge.comprehension_questions]]
question = "How are commands represented?"
expected_concepts = ["Action", "trait", "dispatch"]
wrong_concepts = ["Command pattern"]
weight = 1.0
```

Run with `--llm-judge` flag:

```bash
cargo run -p benchmark -- run --llm-judge --output ./results
```

Requires the `claude` CLI to be available in your PATH (same as the existing `llm` crate).

## Ground Truth

The benchmark uses a hybrid approach for validation:

### 1. Noise Pattern Detection (Automatic)

Default patterns that identify test/mock code:

**File Patterns:**
- `**/tests/**`, `**/test/**`, `**/__tests__/**`
- `*_test.rs`, `*_test.go`, `*.test.ts`
- `**/fixtures/**`, `**/mocks/**`

**Symbol Patterns:**
- `test_*`, `Test*`, `Mock*`, `Stub*`, `Fake*`
- `_*` (internal/private symbols)

**Content Patterns:**
- `#[test]`, `#[cfg(test)]`
- `describe(`, `it(`, `expect(`

### 2. Manual Annotations (Optional)

JSON files in `crates/benchmark/annotations/<repo>/`:

```json
{
  "scenario_id": "zed-command-system",
  "critical_files": ["crates/gpui/src/action.rs"],
  "critical_symbols": ["Action", "ActionRegistry"],
  "key_locations": ["crates/gpui/src/action.rs:42"],
  "exploration_paths": [
    {
      "start": "Action",
      "through": ["ActionRegistry"],
      "target": "dispatch",
      "max_hops": 3
    }
  ],
  "notes": ["The Action trait is the core abstraction"]
}
```

### 3. Call Graph Analysis

Petgraph-based analysis for:
- Verifying reachability between symbols
- Scoring navigation hints (callers/callees)
- Measuring path lengths

## Reports

### JSON Report

Machine-readable format for CI integration:

```json
{
  "metadata": {
    "timestamp": "2024-01-15T10:30:00Z",
    "version": "0.1.0",
    "git_commit": "abc123",
    "hostname": "benchmark-runner",
    "total_scenarios": 4
  },
  "summary": {
    "passed": 3,
    "failed": 1,
    "pass_rate": 0.75,
    "performance": {
      "avg_search_latency_p50_ms": 45,
      "avg_search_latency_p95_ms": 120,
      "avg_context_latency_p50_ms": 28,
      "total_queries": 42
    },
    "accuracy": {
      "avg_file_recall": 0.82,
      "avg_symbol_recall": 0.78,
      "avg_mrr": 0.58,
      "avg_noise_ratio": 0.18,
      "avg_convergence_rate": 0.75,
      "avg_hint_utility": 0.62,
      "avg_suggestion_quality": 0.55,
      "avg_context_bloat": 0.22,
      "avg_dead_end_ratio": 0.15
    }
  },
  "scenarios": [...]
}
```

### Markdown Report

Human-readable summary with pass/fail indicators:

```markdown
# Benchmark Results

**Run:** 2024-01-15 10:30:00
**Pass Rate:** 75% (3/4)

## Accuracy

| Scenario | File Recall | Symbol Recall | MRR | Noise | Steps |
|----------|-------------|---------------|-----|-------|-------|
| zed-command-system | ✅ 85% | ✅ 78% | ✅ 0.65 | ✅ 15% | ✅ 2 |
| zed-lsp-integration | ✅ 78% | ✅ 72% | ✅ 0.58 | ⚠️ 22% | ✅ 3 |
| vscode-extensions | ❌ 45% | ❌ 40% | ❌ 0.32 | ❌ 35% | ❌ 5 |

## Exploration Quality

| Scenario | Convergence | Nav Efficiency | Hint Utility | Context Bloat | Dead Ends |
|----------|-------------|----------------|--------------|---------------|-----------|
| zed-command-system | ✅ 85% | ✅ 72% | ⚠️ 58% | ✅ 18% | ✅ 10% |
| zed-lsp-integration | ✅ 78% | ✅ 65% | ✅ 68% | ✅ 22% | ✅ 15% |
| vscode-extensions | ❌ 45% | ❌ 35% | ❌ 42% | ❌ 45% | ❌ 38% |
```

### Comparison Report

Regression detection between runs:

```markdown
# Comparison: baseline vs current

## Regressions (threshold: 10%)

| Scenario | Metric | Baseline | Current | Change |
|----------|--------|----------|---------|--------|
| vscode-extensions | file_recall | 0.65 | 0.45 | -30.8% |

## Improvements

| Scenario | Metric | Baseline | Current | Change |
|----------|--------|----------|---------|--------|
| zed-commands | latency_p50 | 65ms | 42ms | -35.4% |
```

## Architecture

```
crates/benchmark/
├── Cargo.toml
├── src/
│   ├── lib.rs                # Public API
│   ├── main.rs               # CLI (ccengram-bench)
│   ├── session.rs            # Multi-step exploration state
│   ├── indexing.rs           # Initial indexing benchmarks
│   ├── repos/
│   │   ├── mod.rs            # Repository management
│   │   ├── registry.rs       # Zed/VSCode configs
│   │   └── clone.rs          # Tarball download & caching
│   ├── scenarios/
│   │   ├── mod.rs            # Scenario loader
│   │   ├── definition.rs     # TOML schema types
│   │   └── runner.rs         # Daemon communication
│   ├── metrics/
│   │   ├── mod.rs            # Metric types
│   │   ├── performance.rs    # Latency, memory, CPU
│   │   └── accuracy.rs       # Recall, noise, MRR
│   ├── ground_truth/
│   │   ├── mod.rs            # Ground truth API
│   │   ├── call_graph.rs     # Petgraph analysis
│   │   ├── patterns.rs       # Noise detection
│   │   └── annotations.rs    # Manual annotations
│   └── reports/
│       ├── mod.rs            # Report generation
│       ├── json.rs           # Machine-readable
│       ├── markdown.rs       # Human-readable
│       └── comparison.rs     # Regression detection
├── scenarios/                # Built-in scenarios
│   ├── zed_commands.toml
│   ├── zed_lsp.toml
│   ├── zed_blind_exploration.toml
│   ├── zed_feature_exploration.toml
│   ├── zed_location_exploration.toml
│   ├── zed_pattern_exploration.toml
│   ├── vscode_extensions.toml
│   ├── vscode_editor.toml
│   └── vscode_blind_exploration.toml
└── annotations/              # Optional ground truth
    ├── zed/
    │   └── zed-command-system.json
    └── vscode/
```

## Writing New Scenarios

### Exploration Query Patterns

Different exploration goals require different query patterns. The benchmark includes scenarios for each major pattern:

#### 1. Feature-Based Exploration
Questions like "How does X work?" - understanding functionality without knowing implementation details.

```toml
[[steps]]
query = "How does file saving work?"
expand_top = 4

[[steps]]
query = "What triggers a save operation?"
depends_on_previous = true

[[steps]]
query = "How are unsaved changes tracked?"
depends_on_previous = true
```

**Use when:** Agent needs to understand a feature's behavior and implementation.

#### 2. Location-Based Exploration
Questions like "Where is X defined?" or "Where would I add Y?" - finding where to modify code.

```toml
[[steps]]
query = "Where are editor commands defined?"
expand_top = 3

[[steps]]
query = "How do I register a new command?"
depends_on_previous = true

[[steps]]
query = "What's the pattern for {{previous.symbol}}?"
depends_on_previous = true
```

**Use when:** Agent needs to add or modify functionality.

#### 3. Pattern-Based Exploration
Questions like "What handles X?" - understanding cross-cutting architectural patterns.

```toml
[[steps]]
query = "What makes HTTP requests to external services?"
expand_top = 5

[[steps]]
query = "How is authentication handled for external APIs?"
depends_on_previous = true

[[steps]]
query = "Where is {{previous.symbol}} configured?"
depends_on_previous = true
```

**Use when:** Agent needs to understand how the codebase handles a concern (auth, logging, errors, etc.).

#### 4. Blind Exploration
Generic questions without codebase-specific terms - testing true discovery capability.

```toml
[[steps]]
query = "What is the main entry point of this application?"

[[steps]]
query = "How is the user interface organized?"
depends_on_previous = true

[[steps]]
query = "What does {{previous.symbol}} do?"
depends_on_previous = true
```

**Use when:** Testing exploration from zero knowledge.

### Step-by-Step Guide

1. **Identify the exploration goal**: What should an agent discover?

2. **Choose a query pattern**: Feature, location, pattern, or blind exploration?

3. **Define expected outcomes**: Which files/symbols are critical?

4. **Create multi-step queries**: How would an agent naturally explore?

5. **Set realistic thresholds**: Based on difficulty level

6. **Tune `expand_top`**: More context (4-5) for broad queries, less (2-3) for focused ones

### Example scenario creation:

```toml
# scenarios/my_new_scenario.toml

[scenario]
id = "my-new-scenario"
name = "Exploring Feature X"
repo = "zed"
difficulty = "medium"
description = "Explore how feature X is implemented and used"

[task]
prompt = "How does feature X work?"
intent = "architectural_discovery"

[expected]
must_find_files = ["**/feature_x.rs", "**/feature_x/**"]
must_find_symbols = ["FeatureX", "init_feature_x"]
noise_patterns = ["**/tests/**"]

[[steps]]
query = "Where is feature X implemented?"
expected_results = 3
expand_top = 4  # Get more context for initial broad query
scope = "code"

[[steps]]
query = "How is FeatureX initialized?"
depends_on_previous = true
expand_top = 3

[[steps]]
query = "What calls {{previous.symbol}}?"
depends_on_previous = true
context_ids = ["{{previous.id}}"]

[success]
min_discovery_score = 0.6
max_noise_ratio = 0.3
max_steps_to_core = 2
min_convergence_rate = 0.6
max_context_bloat = 0.35
min_file_diversity = 0.5
max_time_to_first_relevant_ms = 3000

# Optional: LLM comprehension testing
[llm_judge]
min_comprehension_score = 0.5

[[llm_judge.comprehension_questions]]
question = "How is feature X structured?"
expected_concepts = ["FeatureX", "init", "module"]
wrong_concepts = []
weight = 1.0
```

### Step Configuration Reference

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `query` | string | required | The exploration query |
| `scope` | string | `"all"` | Search scope: `code`, `memory`, `docs`, `all` |
| `expand_top` | int | `3` | Include full context for top N results |
| `expected_results` | int | none | Expected number of results |
| `max_noise_ratio` | float | none | Max acceptable noise for this step |
| `depends_on_previous` | bool | `false` | Enable template resolution |
| `context_ids` | array | none | IDs to fetch context for |

### Success Criteria Reference

| Criterion | Type | Description |
|-----------|------|-------------|
| `min_discovery_score` | float | Minimum file recall (0.0-1.0) |
| `max_noise_ratio` | float | Maximum noise in results |
| `max_steps_to_core` | int | Max steps to find first core result |
| `min_convergence_rate` | float | How quickly discoveries plateau |
| `max_context_bloat` | float | Max % of useless context calls |
| `min_navigation_efficiency` | float | optimal_hops / actual_hops |
| `min_suggestion_quality` | float | % of useful suggestions |
| `max_dead_end_ratio` | float | Max % of wasted queries |
| `max_time_to_first_relevant_ms` | int | Max ms to first useful result |
| `min_file_diversity` | float | Min unique files / top-5 results |

## Troubleshooting

### "Daemon not running" error

```bash
# Start the daemon first
ccengram daemon

# Then run benchmarks
cargo run -p benchmark -- run
```

### Repository download fails

```bash
# Check network connectivity
curl -I https://github.com/zed-industries/zed/archive/refs/tags/v0.220.3.tar.gz

# Force re-download
cargo run -p benchmark -- index --repos zed --force
```

### Low recall scores

1. Check if expected files use correct glob patterns
2. Verify the daemon has indexed the repository
3. Review noise patterns - may be too aggressive

### High noise ratio

1. Add more specific noise patterns to scenario
2. Check if test files are being returned as top results
3. Consider adjusting ranking weights in the daemon
